{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be88ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "import pandas as pd, csv\n",
    "import operator\n",
    "import os,random,math\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors,Word2Vec, FastText\n",
    "from gensim.downloader import base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5fb4554",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"D:\\Study\\自然语言工程\\GoogleNews-vectors-negative300.bin\"  \n",
    "mymodel = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2336a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\Study\\自然语言工程\\高级\\week2\\lab2resources\\lab2resources\\sentence-completion\\Holmes_Training_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc5c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(path)\n",
    "n = len(filenames)\n",
    "random.shuffle(filenames)\n",
    "trainingfiles = filenames[:int(n*0.5)]\n",
    "heldoutfiles = filenames[int(n*0.5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "459f8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lanugage_model:\n",
    "    \n",
    "    def __init__(self,path,filesize,method):\n",
    "        self.words = []\n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    "        self.trigram = {}\n",
    "        self.quadrigram = {}\n",
    "        \n",
    "        self.gram = {}\n",
    "        \n",
    "        self.path = path\n",
    "        self.filesize = filesize\n",
    "        self.method = method\n",
    "        \n",
    "        self.get_words()\n",
    "        self._processfiles()\n",
    "        self._make_unknowns()\n",
    "        self._discount()\n",
    "        self._convert_to_probs()\n",
    "        #self.get_prob()\n",
    "        \n",
    "    def get_words(self):\n",
    "\n",
    "        for file in trainingfiles[:self.filesize]:\n",
    "            print(f\"processing {file}.text \")\n",
    "            try:\n",
    "                with open (os.path.join(path,file)) as instream:\n",
    "                    for line in instream:\n",
    "                        line = line.rstrip()\n",
    "                        if len(line)>0:             \n",
    "                            tokens = tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "                            self.words.append(tokens)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring rest of file\".format(file))\n",
    "                        \n",
    "    \n",
    "    def _processfiles(self):\n",
    "    \n",
    "        for i in self.words:\n",
    "            for j in i:\n",
    "                self.unigram[j] = 0\n",
    "\n",
    "        for i in self.words:\n",
    "            for j in i:\n",
    "                self.unigram[j] += 1\n",
    "\n",
    "        if self.method == \"bigram\":\n",
    "            for i in self.words:\n",
    "                for j in range(len(i)-1):\n",
    "                    self.bigram[i[j]] = {}\n",
    "            for i in self.words:\n",
    "                for j in range(len(i)-1):\n",
    "                    self.bigram[i[j]][i[j+1]] = 0\n",
    "            for i in self.words:\n",
    "                for j in range(len(i)-1):\n",
    "                    self.bigram[i[j]][i[j+1]] += 1\n",
    "                    \n",
    "            self.gram = self.bigram\n",
    "                    \n",
    "        if self.method == \"trigram\":\n",
    "            for i in self.words:\n",
    "                for j in range(len(i)-2):\n",
    "                    self.trigram[i[j]] = {}\n",
    "            for i in self.words:\n",
    "                for j in range(len(i)-2):\n",
    "                    self.trigram[i[j]][i[j+1],i[j+2]] = 0\n",
    "            for i in self.words:        \n",
    "                 for j in range(len(i)-2):\n",
    "                    self.trigram[i[j]][i[j+1],i[j+2]] += 1\n",
    "                    \n",
    "            self.gram = self.trigram\n",
    "                    \n",
    "        if self.method == \"quadrigram\":\n",
    "            for i in self.words:\n",
    "                for j in range(len(i)-3):\n",
    "                    self.quadrigram[i[j]] = {}\n",
    "            for i in self.words:\n",
    "                for j in range(len(i)-3):\n",
    "                    self.quadrigram[i[j]][i[j+1],i[j+2],i[j+3]] = 0\n",
    "            for i in self.words:        \n",
    "                 for j in range(len(i)-3):\n",
    "                    self.quadrigram[i[j]][i[j+1],i[j+2],i[j+3]] += 1\n",
    "                    \n",
    "            self.gram = self.quadrigram\n",
    "            \n",
    "            \n",
    "    def _convert_to_probs(self):\n",
    "        self.unigram = {k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()} \n",
    "        self.gram = {key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.gram.items()}\n",
    "    \n",
    "    def get_prob(self,token,context=\"\"):\n",
    "        \n",
    "        if self.method == \"unigram\":\n",
    "            return self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
    "        else:\n",
    "            gram = self.gram.get(context[-1],self.gram.get(\"__UNK\",{}))\n",
    "            big_p = gram.get(token,gram.get(\"__UNK\",0))\n",
    "            \n",
    "            lmbda = gram[\"__DISCOUNT\"]\n",
    "            \n",
    "            uni_p = self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
    "            #print(big_p,lmbda,uni_p)\n",
    "            p = big_p + lmbda * uni_p            \n",
    "            return p          \n",
    "        \n",
    "    def nextlikely(self,current=\"\"):\n",
    "        blacklist=[\"__START\",\"__DISCOUNT\"]\n",
    "       \n",
    "        if self.method == \"unigram\":\n",
    "            dist = self.unigram\n",
    "        else:\n",
    "            dist = self.gram.get(current,{})\n",
    "    \n",
    "        mostlikely=list(dist.items())\n",
    "        #filter out any undesirable tokens\n",
    "        filtered=[(w,p) for (w,p) in mostlikely if w not in blacklist]\n",
    "        print(current,len(filtered))\n",
    "        #choose one randomly from the top k\n",
    "        words,probdist = zip(*filtered)\n",
    "        res = random.choices(words,probdist)[0]\n",
    "        return res\n",
    "    \n",
    "    def generate(self,end=\"__END\",limit=20):\n",
    "        current=\"__START\"\n",
    "        tokens=[]\n",
    "        while  current != end and len(tokens) < limit:\n",
    "            current=self.nextlikely(current=current)\n",
    "            tokens.append(current)\n",
    "        return \" \".join(tokens[:-1])\n",
    "    \n",
    "    def compute_prob_line(self,line):\n",
    "       \n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        acc=0\n",
    "        for i,token in enumerate(tokens[1:]):\n",
    "            acc += np.log(self.get_prob(token,tokens[:i+1]))\n",
    "        return acc,len(tokens[1:])\n",
    "    \n",
    "    def compute_probability(self):\n",
    "        #computes the probability (and length) of a corpus contained in filenames\n",
    "        \n",
    "        total_p=0\n",
    "        total_N=0\n",
    "        for i,afile in enumerate(heldoutfiles[:self.filesize]):\n",
    "            print(\"Processing file {}:{}\".format(i,afile))\n",
    "            try:\n",
    "                with open(os.path.join(path,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            p,N = self.compute_prob_line(line)\n",
    "                            total_p += p\n",
    "                            total_N += N\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
    "        return total_p,total_N\n",
    "    \n",
    "    def compute_perplexity(self):\n",
    "        \n",
    "        #compute the probability and length of the corpus\n",
    "        #calculate perplexity\n",
    "        #lower perplexity means that the model better explains the data\n",
    "        \n",
    "        p,N = self.compute_probability()\n",
    "        #print(p,N)\n",
    "        pp = np.exp(-p/N)\n",
    "        return pp  \n",
    "    \n",
    "    def _make_unknowns(self,known=2):\n",
    "        unknown = 0\n",
    "        if self.method == \"unigram\"or self.method == \"bigram\":\n",
    "            for (k,v) in list(self.unigram.items()):\n",
    "                if v < known:\n",
    "                    del self.unigram[k]\n",
    "                    self.unigram[\"__UNK\"] = self.unigram.get(\"__UNK\",0) + v\n",
    "            for (k,adict) in list(self.gram.items()):\n",
    "                for (kk,v) in list(adict.items()):\n",
    "                    isknown = self.unigram.get(kk,0)\n",
    "                    if isknown == 0:\n",
    "                        adict[\"__UNK\"] = adict.get(\"__UNK\",0) + v\n",
    "                        del adict[kk]\n",
    "                isknown = self.unigram.get(k,0)\n",
    "                if isknown == 0:\n",
    "                    del self.gram[k]\n",
    "                    current = self.gram.get(\"__UNK\",{})\n",
    "                    current.update(adict)\n",
    "                    self.gram[\"__UNK\"] = current\n",
    "\n",
    "                else:\n",
    "                    self.gram[k] = adict\n",
    "                    \n",
    "        if self.method == \"trigram\"or self.method == \"quadrigram\":\n",
    "            for (k,v) in list(self.unigram.items()):\n",
    "                if v < known:\n",
    "                    del self.unigram[k]\n",
    "                    self.unigram[\"__UNK\"] = self.unigram.get(\"__UNK\",0) + v\n",
    "            for (k,adict) in list(self.gram.items()):\n",
    "                for (kk,v) in list(adict.items()):\n",
    "                    for vv in kk:\n",
    "                        isknown = self.unigram.get(vv,0)\n",
    "                        dels = False\n",
    "                        if isknown == 0:\n",
    "                            adict[\"__UNK\"] = adict.get(\"__UNK\",0) + v\n",
    "                            dels = True\n",
    "                    if dels == True:\n",
    "                        del adict[kk]\n",
    "                isknown = self.unigram.get(k,0)\n",
    "                if isknown == 0:\n",
    "                    del self.gram[k]\n",
    "                    current = self.gram.get(\"__UNK\",{})\n",
    "                    current.update(adict)\n",
    "                    self.gram[\"__UNK\"] = current\n",
    "                    \n",
    "                else:\n",
    "                    self.gram[k] = adict\n",
    "        \n",
    "    def _discount(self,discount=0.75):\n",
    "        #discount each bigram count by a small fixed amount\n",
    "        self.gram={k:{kk:value-discount for (kk,value) in adict.items()}for (k,adict) in self.gram.items()}\n",
    "        \n",
    "        #for each word, store the total amount of the discount so that the total is the same \n",
    "        #i.e., so we are reserving this as probability mass\n",
    "        for k in self.gram.keys():\n",
    "            lamb = len(self.gram[k])\n",
    "            self.gram[k][\"__DISCOUNT\"] = lamb * discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4290bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ANDES10.TXT.text \n",
      "processing TLTTF10.TXT.text \n",
      "processing OPION11.TXT.text \n",
      "processing VFAIR10.TXT.text \n",
      "processing LAIDR10.TXT.text \n",
      "processing TCHMS10.TXT.text \n",
      "processing DOLIT10.TXT.text \n",
      "processing 1ARGN10.TXT.text \n",
      "processing PPDEL10.TXT.text \n",
      "processing CHILC10.TXT.text \n",
      "processing DUGLAS11.TXT.text \n",
      "processing ALADM10.TXT.text \n",
      "processing ALEXB10.TXT.text \n",
      "processing LTPRN10.TXT.text \n",
      "processing 1ADAM10.TXT.text \n",
      "processing WNBRG11.TXT.text \n",
      "processing HFDTR10.TXT.text \n",
      "UnicodeDecodeError processing HFDTR10.TXT: ignoring rest of file\n",
      "processing STRKM10.TXT.text \n",
      "processing SAWYR10.TXT.text \n",
      "processing DGOLD10.TXT.text \n"
     ]
    }
   ],
   "source": [
    "mylm = lanugage_model(path,20,\"trigram\")\n",
    "#mylm.compute_perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1722990",
   "metadata": {},
   "outputs": [],
   "source": [
    "parentdir =  \"D:\\Study\\自然语言工程\\高级\\week2\\lab2resources\\lab2resources\\sentence-completion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7086a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=os.path.join(parentdir,\"testing_data.csv\")\n",
    "answers=os.path.join(parentdir,\"test_answer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0edcf26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>a)</th>\n",
       "      <th>b)</th>\n",
       "      <th>c)</th>\n",
       "      <th>d)</th>\n",
       "      <th>e)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I have it from the same source that you are bo...</td>\n",
       "      <td>crying</td>\n",
       "      <td>instantaneously</td>\n",
       "      <td>residing</td>\n",
       "      <td>matched</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>It was furnished partly as a sitting and partl...</td>\n",
       "      <td>daintily</td>\n",
       "      <td>privately</td>\n",
       "      <td>inadvertently</td>\n",
       "      <td>miserably</td>\n",
       "      <td>comfortably</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>As I descended , my old ally , the _____ , cam...</td>\n",
       "      <td>gods</td>\n",
       "      <td>moon</td>\n",
       "      <td>panther</td>\n",
       "      <td>guard</td>\n",
       "      <td>country-dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>We got off , _____ our fare , and the trap rat...</td>\n",
       "      <td>rubbing</td>\n",
       "      <td>doubling</td>\n",
       "      <td>paid</td>\n",
       "      <td>naming</td>\n",
       "      <td>carrying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>He held in his hand a _____ of blue paper , sc...</td>\n",
       "      <td>supply</td>\n",
       "      <td>parcel</td>\n",
       "      <td>sign</td>\n",
       "      <td>sheet</td>\n",
       "      <td>chorus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                           question        a)  \\\n",
       "0  1  I have it from the same source that you are bo...    crying   \n",
       "1  2  It was furnished partly as a sitting and partl...  daintily   \n",
       "2  3  As I descended , my old ally , the _____ , cam...      gods   \n",
       "3  4  We got off , _____ our fare , and the trap rat...   rubbing   \n",
       "4  5  He held in his hand a _____ of blue paper , sc...    supply   \n",
       "\n",
       "                b)             c)         d)             e)  \n",
       "0  instantaneously       residing    matched        walking  \n",
       "1        privately  inadvertently  miserably    comfortably  \n",
       "2             moon        panther      guard  country-dance  \n",
       "3         doubling           paid     naming       carrying  \n",
       "4           parcel           sign      sheet         chorus  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(questions) as instream:\n",
    "    csvreader=csv.reader(instream)\n",
    "    lines=list(csvreader)\n",
    "qs_df=pd.DataFrame(lines[1:],columns=lines[0])\n",
    "qs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fe52e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left_context(sent_tokens,window,target=\"_____\"):\n",
    "    found=-1\n",
    "    for i,token in enumerate(sent_tokens):\n",
    "        if token==target:\n",
    "            found=i\n",
    "            break \n",
    "            \n",
    "    if found>-1:\n",
    "        return sent_tokens[i-window:i]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8797b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df['tokens']=qs_df['question'].map(tokenize)\n",
    "qs_df['left_context']=qs_df['tokens'].map(lambda x: get_left_context(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fe5fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df.to_csv(\"D:\\Study\\自然语言工程\\高级\\week2\\lab2resources\\lab2resources\\sentence-completion\\data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f791ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=os.path.join(parentdir,\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f725c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class question:\n",
    "    \n",
    "    def __init__(self,aline):\n",
    "        self.fields=aline\n",
    "    \n",
    "    def get_field(self,field):\n",
    "        return self.fields[question.colnames[field]]\n",
    "    \n",
    "    def get_context(self,field):\n",
    "        left = eval(self.fields[question.colnames[\"left_context\"]])\n",
    "        option = self.fields[question.colnames[field]]\n",
    "        left.append(option)\n",
    "        return left\n",
    "        \n",
    "    def add_answer(self,fields):\n",
    "        self.answer=fields[1]\n",
    "   \n",
    "    def chooseA(self):\n",
    "        return(\"a\")\n",
    "    \n",
    "    def chooserandom(self):\n",
    "        choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "        return np.random.choice(choices)\n",
    "    \n",
    "    def chooseunigram(self,lm):\n",
    "        choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]      \n",
    "        probs=[lm.unigram.get(self.get_field(ch+\")\"),0) for ch in choices]\n",
    "        maxprob=max(probs)\n",
    "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
    "        #if len(bestchoices)>1:\n",
    "        #    print(\"Randomly choosing from {}\".format(len(bestchoices)))\n",
    "        return np.random.choice(bestchoices)\n",
    "    \n",
    "    def choosebigram(self,lm):\n",
    "        choices = [\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "        probs_total = []\n",
    "        for ch in choices:\n",
    "            context = self.get_context(ch+\")\")\n",
    "            awnser_probs = lm.gram.get(context[1],{})#Get all words related to the probability of the previous word in the answer\n",
    "            #Arrange from large to small according to the probability\n",
    "            vocab_bigram = sorted(awnser_probs.items(),key=lambda x:x[1],reverse =True)\n",
    "            for i in vocab_bigram:\n",
    "                try:\n",
    "                    prob = mymodel.similarity(i[0],context[2])\n",
    "                    probs_total.append(prob)\n",
    "                    break\n",
    "                    \n",
    "                except KeyError:\n",
    "                    continue\n",
    "        try:\n",
    "            maxprob = max(probs_total)\n",
    "            bestchoices=[ch for ch,prob in zip(choices,probs_total) if prob == maxprob]\n",
    "            return np.random.choice(bestchoices)\n",
    "        except ValueError: \n",
    "            return np.random.choice(choices)\n",
    "\n",
    "    def choosetrigram(self,lm):\n",
    "        choices = [\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "        probs_total = []\n",
    "        for ch in choices:\n",
    "            context = self.get_context(ch+\")\")\n",
    "            awnser_probs = lm.gram.get(context[0],{})#Get all words related to the probability of the previous word in the answer\n",
    "            #Arrange from large to small according to the probability\n",
    "            vocab_bigram = sorted(awnser_probs.items(),key=lambda x:x[1],reverse =True)\n",
    "            #Traverse all binary phrases related to the previous word. If the word in the acquisition token appears,\n",
    "            for i in vocab_bigram:\n",
    "                try:\n",
    "                    if i[0][0] == context[1]:\n",
    "                        prob = mymodel.similarity(i[0][1],context[2])\n",
    "                        probs_total.append(prob)\n",
    "                        break\n",
    "                    \n",
    "                except KeyError:\n",
    "                    continue\n",
    "        try:\n",
    "            maxprob = max(probs_total)\n",
    "            bestchoices=[ch for ch,prob in zip(choices,probs_total) if prob == maxprob]\n",
    "            return np.random.choice(bestchoices)\n",
    "        except ValueError: \n",
    "                return np.random.choice(choices)\n",
    "\n",
    "    def choosequadrigram(self,lm):\n",
    "        choices = [\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "        probs_total = []\n",
    "        for ch in choices:\n",
    "            context = self.get_context(ch+\")\")\n",
    "            prob = lm.gram.get(context[0],{}).get((context[1],context[2],context[3]),0)\n",
    "            probs_total.append(prob)\n",
    "        maxprob = max(probs_total)\n",
    "        bestchoices=[ch for ch,prob in zip(choices,probs_total) if prob == maxprob]\n",
    "        return np.random.choice(bestchoices)\n",
    "    \n",
    "    def predict(self,method=\"chooseA\",lm=mylm):\n",
    "        #eventually there will be lots of methods to choose from\n",
    "        if method==\"chooseA\":\n",
    "            return self.chooseA()\n",
    "        elif method==\"random\":\n",
    "            return self.chooserandom()\n",
    "        elif method==\"unigram\":\n",
    "            return self.chooseunigram(lm=lm)\n",
    "        elif method==\"bigram\":\n",
    "            return self.choosebigram(lm=lm)\n",
    "        elif method==\"trigram\":\n",
    "            return self.choosetrigram(lm=lm)\n",
    "        elif method==\"quadrigram\":\n",
    "            return self.choosequadrigram(lm=lm)\n",
    "        \n",
    "    def predict_and_score(self,method=\"chooseA\"):\n",
    "        \n",
    "        #compare prediction according to method with the correct answer\n",
    "        #return 1 or 0 accordingly\n",
    "        prediction=self.predict(method=method)\n",
    "        if prediction ==self.answer:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "226c7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scc_reader:\n",
    "    \n",
    "    def __init__(self,qs=questions,ans=answers):\n",
    "        self.qs=qs\n",
    "        self.ans=ans\n",
    "        self.read_files()\n",
    "        \n",
    "    def read_files(self):\n",
    "        \n",
    "        #read in the question file\n",
    "        with open(self.qs) as instream:\n",
    "            csvreader=csv.reader(instream)\n",
    "            qlines=list(csvreader)\n",
    "        \n",
    "        #store the column names as a reverse index so they can be used to reference parts of the question\n",
    "        question.colnames={item:i for i,item in enumerate(qlines[0])}\n",
    "        \n",
    "        #create a question instance for each line of the file (other than heading line)\n",
    "        self.questions=[question(qline) for qline in qlines[1:]]\n",
    "        \n",
    "        #read in the answer file\n",
    "        with open(self.ans) as instream:\n",
    "            csvreader = csv.reader(instream)\n",
    "            alines=list(csvreader)\n",
    "            \n",
    "        #add answers to questions so predictions can be checked    \n",
    "        for q,aline in zip(self.questions,alines[1:]):\n",
    "            q.add_answer(aline)\n",
    "        \n",
    "    def get_field(self,field):\n",
    "        return [q.get_field(field) for q in self.questions] \n",
    "    \n",
    "    def predict(self,method=\"chooseA\"):\n",
    "        return [q.predict(method=method) for q in self.questions]\n",
    "    \n",
    "    def predict_and_score(self,method=\"chooseA\"):\n",
    "        scores=[q.predict_and_score(method=method) for q in self.questions]\n",
    "        return sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "574efc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCC = scc_reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f04ecaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2567307692307692"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCC.predict_and_score(method=\"unigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e92e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_bigram = SCC.predict_and_score(method=\"bigram\")\n",
    "#print(score_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2f7555e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2375\n"
     ]
    }
   ],
   "source": [
    "score_trigram = SCC.predict_and_score(method=\"trigram\")\n",
    "print(score_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41251f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"Wordvec.txt\",\"a+\") as f:\n",
    "   # f.write(f\"GoogleNews-vectors trigram score: {score_trigram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdabd87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b5d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55738f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b9e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
