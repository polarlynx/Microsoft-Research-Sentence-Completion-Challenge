{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "173c3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nltk import word_tokenize as tokenize\n",
    "import pandas as pd, csv\n",
    "import operator\n",
    "import os,random,math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23d6915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([\"feel'st\", 'it'], 'cold'), (['it', 'cold'], '.'), (['cold', '.'], '__END')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "\n",
    "CONTEXT_SIZE = 2  \n",
    "EMBEDDING_DIM = 10  \n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = [\"__END\",\"__START\"]+tokenize(\"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\")+[\"__END\"]\n",
    "\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the last 3, just so you can see what they look like\n",
    "print(trigrams[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb6ec73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82dd826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5329c219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 522 files in the training directory: D:\\Study\\自然语言工程\\高级\\week2\\lab2resources\\lab2resources\\sentence-completion\\Holmes_Training_Data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "TRAINING_DIR=\"D:\\Study\\自然语言工程\\高级\\week2\\lab2resources\\lab2resources\\sentence-completion\\Holmes_Training_Data\"  #this needs to be the parent directory for the training corpus\n",
    "\n",
    "def get_training_testing(training_dir=TRAINING_DIR,split=0.5):\n",
    "\n",
    "    filenames=os.listdir(training_dir)\n",
    "    n=len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n",
    "    random.seed(53)  #if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index=int(n*split)\n",
    "    return(filenames[:index],filenames[index:])\n",
    "\n",
    "trainingfiles,heldoutfiles=get_training_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c22d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.context_size=context_size\n",
    "        self.hidden_size=128\n",
    "        \n",
    "    def initialise(self):\n",
    "        \n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_dim).to(device)\n",
    "        self.linear1 = nn.Linear(self.context_size * self.embedding_dim, self.hidden_size).to(device)\n",
    "        self.linear2 = nn.Linear(self.hidden_size, self.vocab_size).to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "      \n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "    def get_logprob(self,context,target):\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        #return the logprob of the target word given the context\n",
    "        context_idxs = torch.tensor([self.word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        log_probs = self.forward(context_idxs)\n",
    "        target_idx=torch.tensor(self.word_to_ix[target],dtype=torch.long)\n",
    "        return log_probs.index_select(1,target_idx).item()\n",
    "        \n",
    "        \n",
    "    def nextlikely(self,context):\n",
    "        #sample the distribution of target words given the context\n",
    "        context_idxs = torch.tensor([self.word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        log_probs = self.forward(context_idxs)\n",
    "        probs=[math.exp(x) for x in log_probs.flatten().tolist()]\n",
    "        t=random.choices(list(range(len(probs))),weights=probs,k=1)\n",
    "        return self.ix_to_word[t[0]]\n",
    "    \n",
    "    def generate(self,limit=20):\n",
    "        #generate a sequence of tokens according to the model\n",
    "        tokens=[\"__END\",\"__START\"]\n",
    "        while tokens[-1]!=\"__END\" and len(tokens)<limit:\n",
    "            current=self.nextlikely(tokens[-2:])\n",
    "            tokens.append(current)\n",
    "        return \" \".join(tokens[2:-1])\n",
    "    \n",
    "    def train(self,inputngrams,loss_function=nn.NLLLoss(),lr=0.001,epochs=10):\n",
    "        \n",
    "        optimizer=optim.SGD(self.parameters(),lr=lr)\n",
    "        \n",
    "        losses=[]\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for context, target in inputngrams:\n",
    "\n",
    "                # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "                # into integer indices and wrap them in tensors)\n",
    "                context_idxs = torch.tensor([self.word_to_ix[w] for w in context], dtype=torch.long).to(device)\n",
    "\n",
    "                # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "                # new instance, you need to zero out the gradients from the old\n",
    "                # instance\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Step 3. Run the forward pass, getting log probabilities over next\n",
    "                # words\n",
    "                log_probs = self.forward(context_idxs)\n",
    "\n",
    "                # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "                # word wrapped in a tensor)\n",
    "                wordto_ix = torch.tensor([self.word_to_ix[target]], dtype=torch.long).to(device)\n",
    "                loss = loss_function(log_probs, wordto_ix)\n",
    "\n",
    "                # Step 5. Do the backward pass and update the gradient\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "                total_loss += loss.item()\n",
    "            losses.append(total_loss)\n",
    "            print(\"Completed epoch {} with loss {}\".format(epoch,total_loss))\n",
    "        return losses\n",
    "        \n",
    "    \n",
    "    def train_from_corpus(self,training_dir=TRAINING_DIR,files=[]):\n",
    "        alltokens=[\"__END\"]\n",
    "        #reading corpus and tokenize\n",
    "        for afile in files:\n",
    "            print(\"Reading {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "                            alltokens+=tokens\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError reading {}: ignoring file\".format(afile))\n",
    "        \n",
    "        \n",
    "        #get the vocab and build the indexes\n",
    "        self.vocab = set(alltokens)\n",
    "        self.word_to_ix = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.ix_to_word = {i: word for i, word in enumerate(self.vocab)}\n",
    "        \n",
    "        #MUST SET THE VOCAB SIZE and INITIALISE THE NN\n",
    "        self.vocab_size=len(self.vocab) \n",
    "        print(\"Vocabulary size is {}\".format(self.vocab_size))\n",
    "        self.initialise()\n",
    "        \n",
    "        #convert to trigrams\n",
    "        trigrams = [([alltokens[i], alltokens[i + 1]], alltokens[i + 2])\n",
    "            for i in range(len(alltokens) - 2)]\n",
    "        \n",
    "        print(\"Starting training\")\n",
    "        #train using the trigrams\n",
    "        self.train(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "663a2fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DYNMT10.TXT\n",
      "Vocabulary size is 9347\n",
      "Starting training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_90920/53466952.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mMAX_FILES\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNGramLanguageModeler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCONTEXT_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_from_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainingfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mMAX_FILES\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_90920/1103809197.py\u001b[0m in \u001b[0;36mtrain_from_corpus\u001b[1;34m(self, training_dir, files)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m#train using the trigrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrigrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_90920/1103809197.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, inputngrams, loss_function, lr, epochs)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;31m# Step 3. Run the forward pass, getting log probabilities over next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[1;31m# words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                 \u001b[0mlog_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_idxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[1;31m# Step 4. Compute your loss function. (Again, Torch wants the target\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_90920/1103809197.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0membeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_FILES=1\n",
    "model = NGramLanguageModeler(EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "model.train_from_corpus(files=trainingfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8115eb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = [663424.6739484835,597613.0026421341,580916.226916865,569946.9342714975,561527.4023481769,554543.3517483986,548449.8168717449,542956.8089499564,537906.380108112,533183.2371414312]\n",
    "fig = plt.figure(1)\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(loss)\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Losses-line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceda680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8a15e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
