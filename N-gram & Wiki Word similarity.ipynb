{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "03eae922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "import pandas as pd, csv\n",
    "import operator\n",
    "import os,random,math\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors,Word2Vec, FastText\n",
    "from gensim.downloader import base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f3137919",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r\"D:\\Study\\自然语言工程\\fasttext-wiki-news-subwords-300\"\n",
    "mymodel = KeyedVectors.load_word2vec_format(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4f2195ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\Study\\自然语言工程\\高级\\week2\\lab2resources\\lab2resources\\sentence-completion\\Holmes_Training_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bd01a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(path)\n",
    "n = len(filenames)\n",
    "random.shuffle(filenames)\n",
    "trainingfiles = filenames[:int(n*0.5)]\n",
    "heldoutfiles = filenames[int(n*0.5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1aa0b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lanugage_model:\n",
    "    \n",
    "    def __init__(self,path,filesize,method):\n",
    "        self.words = []\n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    "        self.trigram = {}\n",
    "        self.quadrigram = {}\n",
    "        \n",
    "        self.gram = {}\n",
    "        \n",
    "        self.path = path\n",
    "        self.filesize = filesize\n",
    "        self.method = method\n",
    "        \n",
    "        self.get_words()\n",
    "        self._processfiles()\n",
    "        self._make_unknowns()\n",
    "        self._discount()\n",
    "        self._convert_to_probs()\n",
    "        #self.get_prob()\n",
    "        \n",
    "    def get_words(self):\n",
    "\n",
    "        for file in trainingfiles[:self.filesize]:\n",
    "            print(f\"processing {file}.text \")\n",
    "            try:\n",
    "                with open (os.path.join(path,file)) as instream:\n",
    "                    for line in instream:\n",
    "                        line = line.rstrip()\n",
    "                        if len(line)>0:             \n",
    "                            tokens = tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "                            self.words.append(tokens)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring rest of file\".format(file))\n",
    "                        \n",
    "    \n",
    "    def _processfiles(self):\n",
    "    \n",
    "        for i in self.words:\n",
    "            for j in i:\n",
    "                self.unigram[j] = 0\n",
    "\n",
    "        for i in self.words:\n",
    "            for j in i:\n",
    "                self.unigram[j] += 1\n",
    "\n",
    "        if self.method == \"bigram\":\n",
    "            for i in self.words:\n",
    "                for j in range(len(i)-1):\n",
    "                    self.bigram[i[j]] = {}\n",
    "            for i in self.words:\n",
    "                for j in range(len(i)-1):\n",
    "                    self.bigram[i[j]][i[j+1]] = 0\n",
    "            for i in self.words:\n",
    "                for j in range(len(i)-1):\n",
    "                    self.bigram[i[j]][i[j+1]] += 1\n",
    "                    \n",
    "            self.gram = self.bigram\n",
    "                    \n",
    "        if self.method == \"trigram\":\n",
    "            for i in self.words:\n",
    "                for j in range(len(i)-2):\n",
    "                    self.trigram[i[j]] = {}\n",
    "            for i in self.words:\n",
    "                for j in range(len(i)-2):\n",
    "                    self.trigram[i[j]][i[j+1],i[j+2]] = 0\n",
    "            for i in self.words:        \n",
    "                 for j in range(len(i)-2):\n",
    "                    self.trigram[i[j]][i[j+1],i[j+2]] += 1\n",
    "                    \n",
    "            self.gram = self.trigram\n",
    "                    \n",
    "        if self.method == \"quadrigram\":\n",
    "            for i in self.words:\n",
    "                for j in range(len(i)-3):\n",
    "                    self.quadrigram[i[j]] = {}\n",
    "            for i in self.words:\n",
    "                for j in range(len(i)-3):\n",
    "                    self.quadrigram[i[j]][i[j+1],i[j+2],i[j+3]] = 0\n",
    "            for i in self.words:        \n",
    "                 for j in range(len(i)-3):\n",
    "                    self.quadrigram[i[j]][i[j+1],i[j+2],i[j+3]] += 1\n",
    "                    \n",
    "            self.gram = self.quadrigram\n",
    "            \n",
    "            \n",
    "    def _convert_to_probs(self):\n",
    "        self.unigram = {k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()} \n",
    "        self.gram = {key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.gram.items()}\n",
    "    \n",
    "    def get_prob(self,token,context=\"\"):\n",
    "        \n",
    "        if self.method == \"unigram\":\n",
    "            return self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
    "        else:\n",
    "            gram = self.gram.get(context[-1],self.gram.get(\"__UNK\",{}))\n",
    "            big_p = gram.get(token,gram.get(\"__UNK\",0))\n",
    "            \n",
    "            lmbda = gram[\"__DISCOUNT\"]\n",
    "            \n",
    "            uni_p = self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
    "            #print(big_p,lmbda,uni_p)\n",
    "            p = big_p + lmbda * uni_p            \n",
    "            return p          \n",
    "        \n",
    "    def nextlikely(self,current=\"\"):\n",
    "        blacklist=[\"__START\",\"__DISCOUNT\"]\n",
    "       \n",
    "        if self.method == \"unigram\":\n",
    "            dist = self.unigram\n",
    "        else:\n",
    "            dist = self.gram.get(current,{})\n",
    "    \n",
    "        mostlikely=list(dist.items())\n",
    "        #filter out any undesirable tokens\n",
    "        filtered=[(w,p) for (w,p) in mostlikely if w not in blacklist]\n",
    "        print(current,len(filtered))\n",
    "        #choose one randomly from the top k\n",
    "        words,probdist = zip(*filtered)\n",
    "        res = random.choices(words,probdist)[0]\n",
    "        return res\n",
    "    \n",
    "    def generate(self,end=\"__END\",limit=20):\n",
    "        current=\"__START\"\n",
    "        tokens=[]\n",
    "        while  current != end and len(tokens) < limit:\n",
    "            current=self.nextlikely(current=current)\n",
    "            tokens.append(current)\n",
    "        return \" \".join(tokens[:-1])\n",
    "    \n",
    "    def compute_prob_line(self,line):\n",
    "       \n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        acc=0\n",
    "        for i,token in enumerate(tokens[1:]):\n",
    "            acc += np.log(self.get_prob(token,tokens[:i+1]))\n",
    "        return acc,len(tokens[1:])\n",
    "    \n",
    "    def compute_probability(self):\n",
    "        #computes the probability (and length) of a corpus contained in filenames\n",
    "        \n",
    "        total_p=0\n",
    "        total_N=0\n",
    "        for i,afile in enumerate(heldoutfiles[:self.filesize]):\n",
    "            print(\"Processing file {}:{}\".format(i,afile))\n",
    "            try:\n",
    "                with open(os.path.join(path,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            p,N = self.compute_prob_line(line)\n",
    "                            total_p += p\n",
    "                            total_N += N\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
    "        return total_p,total_N\n",
    "    \n",
    "    def compute_perplexity(self):\n",
    "        \n",
    "        #compute the probability and length of the corpus\n",
    "        #calculate perplexity\n",
    "        #lower perplexity means that the model better explains the data\n",
    "        \n",
    "        p,N = self.compute_probability()\n",
    "        #print(p,N)\n",
    "        pp = np.exp(-p/N)\n",
    "        return pp  \n",
    "    \n",
    "    def _make_unknowns(self,known=2):\n",
    "        unknown = 0\n",
    "        if self.method == \"unigram\"or self.method == \"bigram\":\n",
    "            for (k,v) in list(self.unigram.items()):\n",
    "                if v < known:\n",
    "                    del self.unigram[k]\n",
    "                    self.unigram[\"__UNK\"] = self.unigram.get(\"__UNK\",0) + v\n",
    "            for (k,adict) in list(self.gram.items()):\n",
    "                for (kk,v) in list(adict.items()):\n",
    "                    isknown = self.unigram.get(kk,0)\n",
    "                    if isknown == 0:\n",
    "                        adict[\"__UNK\"] = adict.get(\"__UNK\",0) + v\n",
    "                        del adict[kk]\n",
    "                isknown = self.unigram.get(k,0)\n",
    "                if isknown == 0:\n",
    "                    del self.gram[k]\n",
    "                    current = self.gram.get(\"__UNK\",{})\n",
    "                    current.update(adict)\n",
    "                    self.gram[\"__UNK\"] = current\n",
    "\n",
    "                else:\n",
    "                    self.gram[k] = adict\n",
    "                    \n",
    "        if self.method == \"trigram\"or self.method == \"quadrigram\":\n",
    "            for (k,v) in list(self.unigram.items()):\n",
    "                if v < known:\n",
    "                    del self.unigram[k]\n",
    "                    self.unigram[\"__UNK\"] = self.unigram.get(\"__UNK\",0) + v\n",
    "            for (k,adict) in list(self.gram.items()):\n",
    "                for (kk,v) in list(adict.items()):\n",
    "                    for vv in kk:\n",
    "                        isknown = self.unigram.get(vv,0)\n",
    "                        dels = False\n",
    "                        if isknown == 0:\n",
    "                            adict[\"__UNK\"] = adict.get(\"__UNK\",0) + v\n",
    "                            dels = True\n",
    "                    if dels == True:\n",
    "                        del adict[kk]\n",
    "                isknown = self.unigram.get(k,0)\n",
    "                if isknown == 0:\n",
    "                    del self.gram[k]\n",
    "                    current = self.gram.get(\"__UNK\",{})\n",
    "                    current.update(adict)\n",
    "                    self.gram[\"__UNK\"] = current\n",
    "                    \n",
    "                else:\n",
    "                    self.gram[k] = adict\n",
    "        \n",
    "    def _discount(self,discount=0.75):\n",
    "        #discount each bigram count by a small fixed amount\n",
    "        self.gram={k:{kk:value-discount for (kk,value) in adict.items()}for (k,adict) in self.gram.items()}\n",
    "        \n",
    "        #for each word, store the total amount of the discount so that the total is the same \n",
    "        #i.e., so we are reserving this as probability mass\n",
    "        for k in self.gram.keys():\n",
    "            lamb = len(self.gram[k])\n",
    "            self.gram[k][\"__DISCOUNT\"] = lamb * discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f5c1372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing TYIFC10.TXT.text \n",
      "processing TETHR10.TXT.text \n",
      "processing 1BOON10.TXT.text \n",
      "processing MARKT10.TXT.text \n",
      "processing CDRPR10.TXT.text \n",
      "processing MORLL10.TXT.text \n",
      "processing EMMA10.TXT.text \n",
      "processing SARAC10.TXT.text \n",
      "processing TBSCC10.TXT.text \n",
      "processing TDITW10.TXT.text \n"
     ]
    }
   ],
   "source": [
    "mylm = lanugage_model(path,10,\"trigram\")\n",
    "#mylm.compute_perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "bc8e8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parentdir =  \"D:\\Study\\自然语言工程\\高级\\week2\\lab2resources\\lab2resources\\sentence-completion\"\n",
    "\n",
    "questions=os.path.join(parentdir,\"testing_data.csv\")\n",
    "answers=os.path.join(parentdir,\"test_answer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a97f2dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>a)</th>\n",
       "      <th>b)</th>\n",
       "      <th>c)</th>\n",
       "      <th>d)</th>\n",
       "      <th>e)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I have it from the same source that you are bo...</td>\n",
       "      <td>crying</td>\n",
       "      <td>instantaneously</td>\n",
       "      <td>residing</td>\n",
       "      <td>matched</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>It was furnished partly as a sitting and partl...</td>\n",
       "      <td>daintily</td>\n",
       "      <td>privately</td>\n",
       "      <td>inadvertently</td>\n",
       "      <td>miserably</td>\n",
       "      <td>comfortably</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>As I descended , my old ally , the _____ , cam...</td>\n",
       "      <td>gods</td>\n",
       "      <td>moon</td>\n",
       "      <td>panther</td>\n",
       "      <td>guard</td>\n",
       "      <td>country-dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>We got off , _____ our fare , and the trap rat...</td>\n",
       "      <td>rubbing</td>\n",
       "      <td>doubling</td>\n",
       "      <td>paid</td>\n",
       "      <td>naming</td>\n",
       "      <td>carrying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>He held in his hand a _____ of blue paper , sc...</td>\n",
       "      <td>supply</td>\n",
       "      <td>parcel</td>\n",
       "      <td>sign</td>\n",
       "      <td>sheet</td>\n",
       "      <td>chorus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                           question        a)  \\\n",
       "0  1  I have it from the same source that you are bo...    crying   \n",
       "1  2  It was furnished partly as a sitting and partl...  daintily   \n",
       "2  3  As I descended , my old ally , the _____ , cam...      gods   \n",
       "3  4  We got off , _____ our fare , and the trap rat...   rubbing   \n",
       "4  5  He held in his hand a _____ of blue paper , sc...    supply   \n",
       "\n",
       "                b)             c)         d)             e)  \n",
       "0  instantaneously       residing    matched        walking  \n",
       "1        privately  inadvertently  miserably    comfortably  \n",
       "2             moon        panther      guard  country-dance  \n",
       "3         doubling           paid     naming       carrying  \n",
       "4           parcel           sign      sheet         chorus  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(questions) as instream:\n",
    "    csvreader=csv.reader(instream)\n",
    "    lines=list(csvreader)\n",
    "qs_df=pd.DataFrame(lines[1:],columns=lines[0])\n",
    "qs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c4389b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left_context(sent_tokens,window,target=\"_____\"):\n",
    "    found=-1\n",
    "    for i,token in enumerate(sent_tokens):\n",
    "        if token==target:\n",
    "            found=i\n",
    "            break \n",
    "            \n",
    "    if found>-1:\n",
    "        return sent_tokens[i-window:i]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fb19c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df['tokens']=qs_df['question'].map(tokenize)\n",
    "qs_df['left_context']=qs_df['tokens'].map(lambda x: get_left_context(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0ad6f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df.to_csv(\"D:\\Study\\自然语言工程\\高级\\week2\\lab2resources\\lab2resources\\sentence-completion\\data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "05cc961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=os.path.join(parentdir,\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6ed6b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class question:\n",
    "    \n",
    "    def __init__(self,aline):\n",
    "        self.fields=aline\n",
    "    \n",
    "    def get_field(self,field):\n",
    "        return self.fields[question.colnames[field]]\n",
    "    \n",
    "    def get_context(self,field):\n",
    "        left = eval(self.fields[question.colnames[\"left_context\"]])\n",
    "        option = self.fields[question.colnames[field]]\n",
    "        left.append(option)\n",
    "        return left\n",
    "        \n",
    "    def add_answer(self,fields):\n",
    "        self.answer=fields[1]\n",
    "   \n",
    "    def chooseA(self):\n",
    "        return(\"a\")\n",
    "    \n",
    "    def chooserandom(self):\n",
    "        choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "        return np.random.choice(choices)\n",
    "    \n",
    "    def chooseunigram(self,lm):\n",
    "        choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]      \n",
    "        probs=[lm.unigram.get(self.get_field(ch+\")\"),0) for ch in choices]\n",
    "        maxprob=max(probs)\n",
    "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
    "        #if len(bestchoices)>1:\n",
    "        #    print(\"Randomly choosing from {}\".format(len(bestchoices)))\n",
    "        return np.random.choice(bestchoices)\n",
    "    \n",
    "    def choosebigram(self,lm):\n",
    "        choices = [\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "        probs_total = []\n",
    "        for ch in choices:\n",
    "            context = self.get_context(ch+\")\")\n",
    "            awnser_probs = lm.gram.get(context[1],{})#获取答案前一个词相关概率的所有词\n",
    "            vocab_bigram = sorted(awnser_probs.items(),key=lambda x:x[1],reverse =True)#根据概率从大到小排列\n",
    "            for i in vocab_bigram:\n",
    "                try:\n",
    "                    prob = mymodel.similarity(i[0],context[2])\n",
    "                    probs_total.append(prob)\n",
    "                    break\n",
    "                    \n",
    "                except KeyError:\n",
    "                    continue\n",
    "        try:\n",
    "            maxprob = max(probs_total)\n",
    "            bestchoices=[ch for ch,prob in zip(choices,probs_total) if prob == maxprob]\n",
    "            return np.random.choice(bestchoices)\n",
    "        except ValueError: \n",
    "            return np.random.choice(choices)\n",
    "\n",
    "    def choosetrigram(self,lm):\n",
    "        choices = [\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "        probs_total = []\n",
    "        for ch in choices:\n",
    "            context = self.get_context(ch+\")\")\n",
    "            awnser_probs = lm.gram.get(context[0],{})#获取答案前一个词相关概率的所有词\n",
    "            vocab_bigram = sorted(awnser_probs.items(),key=lambda x:x[1],reverse =True)#根据概率从大到小排列\n",
    "            #遍历和前一个词相关的所有二元词组，如果\n",
    "            for i in vocab_bigram:\n",
    "                try:\n",
    "                    if i[0][0] == context[1]:\n",
    "                        prob = mymodel.similarity(i[0][1],context[2])\n",
    "                        probs_total.append(prob)\n",
    "                        break\n",
    "                    \n",
    "                except KeyError:\n",
    "                    continue\n",
    "        try:\n",
    "            maxprob = max(probs_total)\n",
    "            bestchoices=[ch for ch,prob in zip(choices,probs_total) if prob == maxprob]\n",
    "            return np.random.choice(bestchoices)\n",
    "        except ValueError: \n",
    "                return np.random.choice(choices)\n",
    "\n",
    "    def choosequadrigram(self,lm):\n",
    "        choices = [\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "        probs_total = []\n",
    "        for ch in choices:\n",
    "            context = self.get_context(ch+\")\")\n",
    "            prob = lm.gram.get(context[0],{}).get((context[1],context[2],context[3]),0)\n",
    "            probs_total.append(prob)\n",
    "        maxprob = max(probs_total)\n",
    "        bestchoices=[ch for ch,prob in zip(choices,probs_total) if prob == maxprob]\n",
    "        return np.random.choice(bestchoices)\n",
    "    \n",
    "    def predict(self,method=\"chooseA\",lm=mylm):\n",
    "        #eventually there will be lots of methods to choose from\n",
    "        if method==\"chooseA\":\n",
    "            return self.chooseA()\n",
    "        elif method==\"random\":\n",
    "            return self.chooserandom()\n",
    "        elif method==\"unigram\":\n",
    "            return self.chooseunigram(lm=lm)\n",
    "        elif method==\"bigram\":\n",
    "            return self.choosebigram(lm=lm)\n",
    "        elif method==\"trigram\":\n",
    "            return self.choosetrigram(lm=lm)\n",
    "        elif method==\"quadrigram\":\n",
    "            return self.choosequadrigram(lm=lm)\n",
    "        \n",
    "    def predict_and_score(self,method=\"chooseA\"):\n",
    "        \n",
    "        #compare prediction according to method with the correct answer\n",
    "        #return 1 or 0 accordingly\n",
    "        prediction=self.predict(method=method)\n",
    "        if prediction ==self.answer:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "da6a24ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scc_reader:\n",
    "    \n",
    "    def __init__(self,qs=questions,ans=answers):\n",
    "        self.qs=qs\n",
    "        self.ans=ans\n",
    "        self.read_files()\n",
    "        \n",
    "    def read_files(self):\n",
    "        \n",
    "        #read in the question file\n",
    "        with open(self.qs) as instream:\n",
    "            csvreader=csv.reader(instream)\n",
    "            qlines=list(csvreader)\n",
    "        \n",
    "        #store the column names as a reverse index so they can be used to reference parts of the question\n",
    "        question.colnames={item:i for i,item in enumerate(qlines[0])}\n",
    "        \n",
    "        #create a question instance for each line of the file (other than heading line)\n",
    "        self.questions=[question(qline) for qline in qlines[1:]]\n",
    "        \n",
    "        #read in the answer file\n",
    "        with open(self.ans) as instream:\n",
    "            csvreader = csv.reader(instream)\n",
    "            alines=list(csvreader)\n",
    "            \n",
    "        #add answers to questions so predictions can be checked    \n",
    "        for q,aline in zip(self.questions,alines[1:]):\n",
    "            q.add_answer(aline)\n",
    "        \n",
    "    def get_field(self,field):\n",
    "        return [q.get_field(field) for q in self.questions] \n",
    "    \n",
    "    def predict(self,method=\"chooseA\"):\n",
    "        return [q.predict(method=method) for q in self.questions]\n",
    "    \n",
    "    def predict_and_score(self,method=\"chooseA\"):\n",
    "        scores=[q.predict_and_score(method=method) for q in self.questions]\n",
    "        return sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "32f8e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCC = scc_reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "689eea35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2519230769230769"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCC.predict_and_score(method=\"unigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "36f67498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_bigram = SCC.predict_and_score(method=\"bigram\")\n",
    "#print(score_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "986fd14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24519230769230768\n"
     ]
    }
   ],
   "source": [
    "score_trigram = SCC.predict_and_score(method=\"trigram\")\n",
    "print(score_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09551235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8113e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Wordvec.txt\",\"a+\") as f:\n",
    "    f.write(f\"WikiNews-vectors trigram score: {score_trigram}  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128d313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af23a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
